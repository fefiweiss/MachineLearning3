{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # 4. Reconocimiento de Imágenes Sign Gestures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejercicio se utilizará el dataset de Sign Gestures el que consiste en 32627 imagenes donde en cada una hay una letra en lenguaje de señas. El dataset viene separado en 27455 ejemplos de entrenamiento y 7172 de pruebas. Las clases son mutuamente excluyentes y las imagenes tienen una resolución de 28x28 pixeles representados en una escala de grises 0-255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conjunto de entrenamiento:  (21964, 784)\n",
      "Conjunto de validación:  (5491, 784)\n",
      "Conjunto de entrenamiento:  (7172, 784)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def load_data():\n",
    "    train = pd.read_csv('sign_mnist_train.csv')\n",
    "    test = pd.read_csv('sign_mnist_test.csv')\n",
    "    y_tr = train['label']\n",
    "    x_tr = train.iloc[:,1:]\n",
    "    delete_from_train = int(0.2*len(x_tr))\n",
    "    validate = len(x_tr) - delete_from_train\n",
    "    #you need to add Xval: x_v,y_v\n",
    "    x_v = train.iloc[validate:len(x_tr),1:]\n",
    "    y_v = train['label'][validate:len(x_tr)]\n",
    "    y_tr = y_tr.drop(y_tr.tail(delete_from_train).index)\n",
    "    x_tr = x_tr.drop(x_tr.tail(delete_from_train).index)\n",
    "    y_t = test['label']\n",
    "    x_t = test.iloc[:,1:]\n",
    "    \n",
    "    \n",
    "    return(x_tr,x_v,x_t,y_tr,y_v,y_t)\n",
    "x_tr, x_v, x_t, y_tr, y_v , y_t= load_data()\n",
    "print(\"Conjunto de entrenamiento: \",x_tr.shape)\n",
    "print(\"Conjunto de validación: \",x_v.shape)\n",
    "print(\"Conjunto de entrenamiento: \",x_t.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la celda anterior, se generaron 2 matrices corespondiente a las imágenes y etiquetas de entrenamiento : $X_{tr},y_{tr}$ ambos poseen 21964 elementos. Se dividió este dataset para crear el conjunto de validación, el cual corresponde al 20% de elementos del conjunto de entrenamiento, por lo tanto, $X_{v},y_{v}$ poseen 5491 elementos. El conjunto de pruebas $X_{t},y_{t}$ en el cual se evaluarán los distintos algoritmos está compuesto de 7172. Cabe recordar que cada imagen está guardada como un arreglo de 28x28 , esto es de 784 pixeles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Pre-procesamiento de imágenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente celda se construye una función que tiene 2 opciones, una consiste en **sólo escalar** los datos, esto es dividir por la intensidad máxima de pixel en el dataset, la otra **centra y escala** los datos según su varianza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalar y centrar datos\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def scaler_t(X,Xv,Xt,scaler_type):\n",
    "    if scaler_type == \"escalar\":\n",
    "        scaler= StandardScaler(with_std=False).fit(X)\n",
    "        print(\"Se han escalado las imágenes\")\n",
    "        return scaler.transform(X), scaler.transform(Xv), scaler.transform(Xt)\n",
    "    elif scaler_type == \"centrar_escalar\":\n",
    "        scaler= StandardScaler(with_std=True).fit(X)\n",
    "        print(\"Se han centrado y escalado las imágenes\")\n",
    "        return scaler.transform(X), scaler.transform(Xv), scaler.transform(Xt)\n",
    "    else: #otro\n",
    "        print(\"Opción no registrada\")\n",
    "\n",
    "#Buscar como visualizar imagen!\n",
    "#ver_imagen(X,Y_train,20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Dise~ne, entrene y eval\u0013ue una red neuronal para el problema partir de la representaci\u0013on original de las\n",
    "im\u0013agenes. Experimente con distintas arquitecturas, pre-procesamientos y m\u0013etodos de entrenamiento,\n",
    "midiendo el error de clasi\f",
    "caci\u0013on sobre el conjunto de validaci\u0013on. En base a esta \u0013ultima medida de\n",
    "desempe~no, decida qu\u0013e modelo, de entre todos los evaluados, medir\u0013a \f",
    "nalmente en el conjunto de test.\n",
    "Reporte y discuta los resultados obtenidos. Se espera que logre obtener un error de pruebas menor o\n",
    "igual a 0:2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Diseñar y entrenar una red neuronal con error de pruebas < 0.2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este item se pide diseñar, entrenar y evaluar una red con distintas representaciones y arquitecturas con tal de obtener un error menor al 20% en el conjunto de prueba. Para analizar cual de los modelos probar, se utilizará el conjunto de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se han centrado y escalado las imágenes\n"
     ]
    }
   ],
   "source": [
    "# Creacion de conjuntos centrados y escalados.\n",
    "px_tr,px_v,px_t = scaler_t(x_tr,x_v,x_t,\"centrar_escalar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primera instancia analizaremos cómo se comporta la red con la representación **original** , con una configuración con 3 capas ocultas, y función de pérdida categorical_crossentropy por el carácter multilabel de la clasificación, se realizarán 100 epochs : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, input_dim=784, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  import sys\n",
      "c:\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  \n",
      "c:\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:9: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(25, activation=\"softmax\", kernel_initializer=\"uniform\")`\n",
      "  if __name__ == '__main__':\n",
      "c:\\anaconda3\\lib\\site-packages\\keras\\models.py:939: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "21964/21964 [==============================] - 8s 344us/step - loss: 15.2950 - acc: 0.0445\n",
      "Epoch 2/50\n",
      "21964/21964 [==============================] - 4s 186us/step - loss: 15.3931 - acc: 0.0450\n",
      "Epoch 3/50\n",
      "21964/21964 [==============================] - 4s 187us/step - loss: 15.3931 - acc: 0.0450\n",
      "Epoch 4/50\n",
      "21964/21964 [==============================] - 4s 191us/step - loss: 15.3931 - acc: 0.0450\n",
      "Epoch 5/50\n",
      "21964/21964 [==============================] - 4s 190us/step - loss: 15.3931 - acc: 0.0450\n",
      "Epoch 6/50\n",
      "21964/21964 [==============================] - 4s 187us/step - loss: 15.3931 - acc: 0.0450\n",
      "Epoch 7/50\n",
      "21964/21964 [==============================] - 4s 189us/step - loss: 15.3931 - acc: 0.0450\n",
      "Epoch 8/50\n",
      "21964/21964 [==============================] - 4s 193us/step - loss: 15.3931 - acc: 0.0450\n",
      "Epoch 9/50\n",
      "21964/21964 [==============================] - 4s 193us/step - loss: 15.3931 - acc: 0.0450\n",
      "Epoch 10/50\n",
      "21964/21964 [==============================] - 4s 188us/step - loss: 15.3931 - acc: 0.0450\n",
      "Epoch 11/50\n",
      "21964/21964 [==============================] - 4s 193us/step - loss: 15.3931 - acc: 0.0450\n",
      "Epoch 12/50\n",
      "21964/21964 [==============================] - 4s 195us/step - loss: 15.3931 - acc: 0.0450\n",
      "Epoch 13/50\n",
      "21964/21964 [==============================] - 4s 200us/step - loss: 15.3931 - acc: 0.0450\n",
      "Epoch 14/50\n",
      "21964/21964 [==============================] - 4s 201us/step - loss: 15.3931 - acc: 0.0450\n",
      "Epoch 15/50\n",
      "21964/21964 [==============================] - 5s 220us/step - loss: 15.3931 - acc: 0.0450\n",
      "Epoch 16/50\n",
      "21964/21964 [==============================] - 5s 226us/step - loss: 15.3931 - acc: 0.0450\n",
      "Epoch 17/50\n",
      "21964/21964 [==============================] - 4s 198us/step - loss: 15.3931 - acc: 0.0450\n",
      "Epoch 18/50\n",
      "21964/21964 [==============================] - 4s 195us/step - loss: 15.3931 - acc: 0.0450\n",
      "Epoch 19/50\n",
      "21964/21964 [==============================] - 4s 193us/step - loss: 15.3931 - acc: 0.0450\n",
      "Epoch 20/50\n",
      "21964/21964 [==============================] - 4s 195us/step - loss: 15.3931 - acc: 0.0450\n",
      "Epoch 21/50\n",
      "21964/21964 [==============================] - 4s 197us/step - loss: 15.3931 - acc: 0.0450\n",
      "Epoch 22/50\n",
      "21964/21964 [==============================] - 4s 187us/step - loss: 15.3931 - acc: 0.0450\n",
      "Epoch 23/50\n",
      "21964/21964 [==============================] - 4s 193us/step - loss: 15.3931 - acc: 0.0450\n",
      "Epoch 24/50\n",
      "21964/21964 [==============================] - 4s 194us/step - loss: 15.3931 - acc: 0.0450\n",
      "Epoch 25/50\n",
      "21964/21964 [==============================] - 4s 201us/step - loss: 15.3931 - acc: 0.04500s - los\n",
      "Epoch 26/50\n",
      "21964/21964 [==============================] - 4s 196us/step - loss: 15.3931 - acc: 0.0450\n",
      "Epoch 27/50\n",
      "21964/21964 [==============================] - 4s 198us/step - loss: 15.3931 - acc: 0.0450\n",
      "Epoch 28/50\n",
      "21964/21964 [==============================] - 4s 192us/step - loss: 15.3931 - acc: 0.0450\n",
      "Epoch 29/50\n",
      "21964/21964 [==============================] - 4s 200us/step - loss: 15.3931 - acc: 0.0450\n",
      "Epoch 30/50\n",
      "21964/21964 [==============================] - 4s 195us/step - loss: 15.3931 - acc: 0.0450\n",
      "Epoch 31/50\n",
      "21964/21964 [==============================] - 4s 193us/step - loss: 15.3931 - acc: 0.0450\n",
      "Epoch 32/50\n",
      "21964/21964 [==============================] - 4s 194us/step - loss: 15.3931 - acc: 0.0450\n",
      "Epoch 33/50\n",
      "21964/21964 [==============================] - 4s 197us/step - loss: 15.3931 - acc: 0.0450\n",
      "Epoch 34/50\n",
      "21964/21964 [==============================] - 4s 197us/step - loss: 15.3931 - acc: 0.0450\n",
      "Epoch 35/50\n",
      "21964/21964 [==============================] - 4s 194us/step - loss: 15.3931 - acc: 0.0450\n",
      "Epoch 36/50\n",
      "21964/21964 [==============================] - 4s 191us/step - loss: 15.3931 - acc: 0.0450\n",
      "Epoch 37/50\n",
      "21964/21964 [==============================] - 4s 193us/step - loss: 15.3931 - acc: 0.0450\n",
      "Epoch 38/50\n",
      "21964/21964 [==============================] - 4s 193us/step - loss: 15.3931 - acc: 0.0450\n",
      "Epoch 39/50\n",
      "21964/21964 [==============================] - 4s 193us/step - loss: 15.3931 - acc: 0.0450\n",
      "Epoch 40/50\n",
      "21964/21964 [==============================] - 4s 192us/step - loss: 15.3931 - acc: 0.0450\n",
      "Epoch 41/50\n",
      "21964/21964 [==============================] - 4s 192us/step - loss: 15.3931 - acc: 0.0450\n",
      "Epoch 42/50\n",
      "21964/21964 [==============================] - 4s 192us/step - loss: 15.3931 - acc: 0.0450\n",
      "Epoch 43/50\n",
      "21964/21964 [==============================] - 4s 189us/step - loss: 15.3931 - acc: 0.0450\n",
      "Epoch 44/50\n",
      "21964/21964 [==============================] - 4s 192us/step - loss: 15.3931 - acc: 0.0450\n",
      "Epoch 45/50\n",
      "21964/21964 [==============================] - 4s 190us/step - loss: 15.3931 - acc: 0.0450\n",
      "Epoch 46/50\n",
      "21964/21964 [==============================] - 4s 194us/step - loss: 15.3931 - acc: 0.0450\n",
      "Epoch 47/50\n",
      "21964/21964 [==============================] - 5s 206us/step - loss: 15.3931 - acc: 0.0450\n",
      "Epoch 48/50\n",
      "21964/21964 [==============================] - 5s 220us/step - loss: 15.3931 - acc: 0.0450\n",
      "Epoch 49/50\n",
      "21964/21964 [==============================] - 5s 219us/step - loss: 15.3931 - acc: 0.0450\n",
      "Epoch 50/50\n",
      "21964/21964 [==============================] - 5s 221us/step - loss: 15.3931 - acc: 0.0450\n",
      "5491/5491 [==============================] - 3s 491us/step\n",
      "test accuracy: 0.052995811329\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim=x_tr.shape[1], init='uniform', activation='relu'))\n",
    "model.add(Dense(30, init='uniform', activation='relu'))\n",
    "model.add(Dense(25, init='uniform', activation='softmax'))\n",
    "model.compile(optimizer=SGD(lr=0.05), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_tr, to_categorical(y_tr), nb_epoch=50, batch_size=128, verbose=1)\n",
    "evaluate = model.evaluate(x_v,to_categorical(y_v)) \n",
    "print(\"test accuracy:\",evaluate[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizaremos el mismo procedimiento pero con la representación centrada y escalada : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, input_dim=784, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  \n",
      "c:\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "c:\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(25, activation=\"softmax\", kernel_initializer=\"uniform\")`\n",
      "  after removing the cwd from sys.path.\n",
      "c:\\anaconda3\\lib\\site-packages\\keras\\models.py:939: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "21964/21964 [==============================] - 7s 305us/step - loss: 3.1989 - acc: 0.0859\n",
      "Epoch 2/50\n",
      "21964/21964 [==============================] - 4s 187us/step - loss: 2.9280 - acc: 0.1365\n",
      "Epoch 3/50\n",
      "21964/21964 [==============================] - 4s 195us/step - loss: 2.2820 - acc: 0.2643\n",
      "Epoch 4/50\n",
      "21964/21964 [==============================] - 4s 201us/step - loss: 1.6295 - acc: 0.4490\n",
      "Epoch 5/50\n",
      "21964/21964 [==============================] - 5s 207us/step - loss: 1.1310 - acc: 0.6212\n",
      "Epoch 6/50\n",
      "21964/21964 [==============================] - 4s 204us/step - loss: 0.7851 - acc: 0.7458\n",
      "Epoch 7/50\n",
      "21964/21964 [==============================] - 5s 210us/step - loss: 0.5462 - acc: 0.8380\n",
      "Epoch 8/50\n",
      "21964/21964 [==============================] - 5s 214us/step - loss: 0.3695 - acc: 0.9036\n",
      "Epoch 9/50\n",
      "21964/21964 [==============================] - 4s 204us/step - loss: 0.2422 - acc: 0.9483\n",
      "Epoch 10/50\n",
      "21964/21964 [==============================] - 5s 211us/step - loss: 0.1583 - acc: 0.9747\n",
      "Epoch 11/50\n",
      "21964/21964 [==============================] - 5s 219us/step - loss: 0.1047 - acc: 0.9886\n",
      "Epoch 12/50\n",
      "21964/21964 [==============================] - 5s 214us/step - loss: 0.0716 - acc: 0.9959 1s - l\n",
      "Epoch 13/50\n",
      "21964/21964 [==============================] - 5s 205us/step - loss: 0.0505 - acc: 0.9985\n",
      "Epoch 14/50\n",
      "21964/21964 [==============================] - 5s 205us/step - loss: 0.0378 - acc: 0.9989\n",
      "Epoch 15/50\n",
      "21964/21964 [==============================] - 5s 210us/step - loss: 0.0292 - acc: 0.9996\n",
      "Epoch 16/50\n",
      "21964/21964 [==============================] - 5s 210us/step - loss: 0.0234 - acc: 0.9997\n",
      "Epoch 17/50\n",
      "21964/21964 [==============================] - 5s 217us/step - loss: 0.0194 - acc: 0.9998\n",
      "Epoch 18/50\n",
      "21964/21964 [==============================] - 5s 217us/step - loss: 0.0163 - acc: 1.0000\n",
      "Epoch 19/50\n",
      "21964/21964 [==============================] - 5s 215us/step - loss: 0.0140 - acc: 1.0000\n",
      "Epoch 20/50\n",
      "21964/21964 [==============================] - 5s 215us/step - loss: 0.0122 - acc: 1.0000\n",
      "Epoch 21/50\n",
      "21964/21964 [==============================] - 5s 213us/step - loss: 0.0107 - acc: 1.0000\n",
      "Epoch 22/50\n",
      "21964/21964 [==============================] - 5s 212us/step - loss: 0.0096 - acc: 1.0000\n",
      "Epoch 23/50\n",
      "21964/21964 [==============================] - 5s 214us/step - loss: 0.0086 - acc: 1.0000\n",
      "Epoch 24/50\n",
      "21964/21964 [==============================] - 5s 211us/step - loss: 0.0078 - acc: 1.0000\n",
      "Epoch 25/50\n",
      "21964/21964 [==============================] - 5s 210us/step - loss: 0.0072 - acc: 1.0000\n",
      "Epoch 26/50\n",
      "21964/21964 [==============================] - 5s 211us/step - loss: 0.0066 - acc: 1.0000\n",
      "Epoch 27/50\n",
      "21964/21964 [==============================] - 5s 210us/step - loss: 0.0061 - acc: 1.0000\n",
      "Epoch 28/50\n",
      "21964/21964 [==============================] - 5s 208us/step - loss: 0.0057 - acc: 1.0000\n",
      "Epoch 29/50\n",
      "21964/21964 [==============================] - 5s 210us/step - loss: 0.0053 - acc: 1.0000\n",
      "Epoch 30/50\n",
      "21964/21964 [==============================] - 5s 207us/step - loss: 0.0049 - acc: 1.0000ETA: 0s - loss: 0.0049 - acc: 1.0\n",
      "Epoch 31/50\n",
      "21964/21964 [==============================] - 5s 207us/step - loss: 0.0046 - acc: 1.0000 0s - loss: 0.0046 - acc: 1.0\n",
      "Epoch 32/50\n",
      "21964/21964 [==============================] - 4s 205us/step - loss: 0.0043 - acc: 1.0000\n",
      "Epoch 33/50\n",
      "21964/21964 [==============================] - 4s 200us/step - loss: 0.0041 - acc: 1.0000\n",
      "Epoch 34/50\n",
      "21964/21964 [==============================] - 4s 203us/step - loss: 0.0039 - acc: 1.0000\n",
      "Epoch 35/50\n",
      "21964/21964 [==============================] - 4s 204us/step - loss: 0.0037 - acc: 1.0000\n",
      "Epoch 36/50\n",
      "21964/21964 [==============================] - 4s 205us/step - loss: 0.0035 - acc: 1.0000\n",
      "Epoch 37/50\n",
      "21964/21964 [==============================] - 4s 201us/step - loss: 0.0033 - acc: 1.0000\n",
      "Epoch 38/50\n",
      "21964/21964 [==============================] - 4s 203us/step - loss: 0.0032 - acc: 1.0000 1s \n",
      "Epoch 39/50\n",
      "21964/21964 [==============================] - 4s 201us/step - loss: 0.0030 - acc: 1.0000\n",
      "Epoch 40/50\n",
      "21964/21964 [==============================] - 4s 204us/step - loss: 0.0029 - acc: 1.0000\n",
      "Epoch 41/50\n",
      "21964/21964 [==============================] - 4s 197us/step - loss: 0.0028 - acc: 1.0000\n",
      "Epoch 42/50\n",
      "21964/21964 [==============================] - 4s 197us/step - loss: 0.0027 - acc: 1.0000\n",
      "Epoch 43/50\n",
      "21964/21964 [==============================] - 4s 194us/step - loss: 0.0026 - acc: 1.0000 0s - loss: 0.0026 - ac\n",
      "Epoch 44/50\n",
      "21964/21964 [==============================] - 4s 196us/step - loss: 0.0025 - acc: 1.0000\n",
      "Epoch 45/50\n",
      "21964/21964 [==============================] - 4s 195us/step - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 46/50\n",
      "21964/21964 [==============================] - 4s 195us/step - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 47/50\n",
      "21964/21964 [==============================] - 4s 186us/step - loss: 0.0022 - acc: 1.0000\n",
      "Epoch 48/50\n",
      "21964/21964 [==============================] - 4s 188us/step - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 49/50\n",
      "21964/21964 [==============================] - 4s 193us/step - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 50/50\n",
      "21964/21964 [==============================] - 5s 208us/step - loss: 0.0020 - acc: 1.0000\n",
      "5491/5491 [==============================] - 2s 405us/step\n",
      "test accuracy: 0.99981788381\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim=px_tr.shape[1], init='uniform', activation='relu'))\n",
    "model.add(Dense(30, init='uniform', activation='relu'))\n",
    "model.add(Dense(25, init='uniform', activation='softmax'))\n",
    "model.compile(optimizer=SGD(lr=0.05), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(px_tr, to_categorical(y_tr), nb_epoch=50, batch_size=128, verbose=1)          \n",
    "evaluate = model.evaluate(px_v,to_categorical(y_v)) \n",
    "print(\"test accuracy:\",evaluate[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7172/7172 [==============================] - 0s 48us/step\n",
      "[2.0101451818595968, 0.68251533742331283]\n",
      "0.99981788381\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "evaluate = model.evaluate(px_t,to_categorical(y_t)) \n",
    "print(evaluate)\n",
    "y_red = model.predict(px_v)\n",
    "y_pred = []\n",
    "for i in range(len(y_red)):\n",
    "    lista = list(y_red[i])\n",
    "    y_pred.append(lista.index(max(y_red[i])))\n",
    "    \n",
    "print(accuracy_score(y_pred,list(y_v)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observando ambas redes, se obtienen mejores resultados con la representación centrada y escalada, por lo tanto, probaremos distintas arquitecturas basadas en esta última representación:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variando tasa de aprendizaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, input_dim=784, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  \n",
      "c:\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(30, activation=\"relu\", kernel_initializer=\"uniform\")`\n",
      "  import sys\n",
      "c:\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(25, activation=\"softmax\", kernel_initializer=\"uniform\")`\n",
      "  \n",
      "c:\\anaconda3\\lib\\site-packages\\keras\\models.py:939: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7172/7172 [==============================] - 0s 58us/step\n",
      "7172/7172 [==============================] - 1s 136us/step\n",
      "7172/7172 [==============================] - 0s 58us/step\n",
      "7172/7172 [==============================] - 1s 85us/step\n",
      "7172/7172 [==============================] - 1s 176us/step\n",
      "7172/7172 [==============================] - 1s 75us/step\n",
      "7172/7172 [==============================] - 1s 198us/step\n",
      "7172/7172 [==============================] - 3s 393us/step\n",
      "7172/7172 [==============================] - 2s 273us/step\n",
      "7172/7172 [==============================] - 2s 242us/step\n",
      "7172/7172 [==============================] - 1s 145us/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "lrs = [0.5,0.51,0.52,0.53,0.54,0.55,0.56,0.57,0.58,0.59,0.6]\n",
    "accuracys = []\n",
    "for tasa in lrs:\n",
    "    model = Sequential()\n",
    "    model.add(Dense(30, input_dim=px_tr.shape[1], init='uniform', activation='relu'))\n",
    "    model.add(Dense(30, init='uniform', activation='relu'))\n",
    "    model.add(Dense(25, init='uniform', activation='softmax'))\n",
    "    model.compile(optimizer=SGD(lr=tasa), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(px_tr, to_categorical(y_tr), nb_epoch=50, batch_size=128, verbose=0,\n",
    "              validation_data=(px_v,to_categorical(y_v)))\n",
    "    evaluate = model.evaluate(px_t,to_categorical(y_t)) \n",
    "    accuracys.append(evaluate[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.75362520914668152, 0.76310652537646406, 0.20524261015058562, 0.73368655883993306, 0.73954266592303397, 0.13301728945900726, 0.18655883993307307, 0.20956497490239823, 0.16564417177914109, 0.15351366424986057, 0.12311767986614612]\n"
     ]
    }
   ],
   "source": [
    "print(accuracys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lr = 0.51! es el mejor experimentalmente\n",
    "Implementar Dropout,\n",
    "\n",
    "Duda => Como hacer validación si da 100% de accuracy!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "* Learning Rate\n",
    "* Dropout\n",
    "* Cantidad de capas/neuronas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Para la mejor red entrenada anteriormente construya la matriz de confusi\u0013on de las distintas clases, para\n",
    "asi visualizar cu\u0013ales son las clases m\u0013as dif\u0013\u0010ciles de clasi\f",
    "car y con cu\u0013ales se confunden. Comente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-e29691481cd6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# print(to_categorical(y_t[:10]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0my_red\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;31m#print(y_red[0:10])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# print(to_categorical(y_t[:10]))\n",
    "y_red = list((model.predict(x_t)))\n",
    "#print(y_red[0:10])\n",
    "y_pred = []\n",
    "for i in range(len(y_red)):\n",
    "    z = [0]*len(y_red[i])\n",
    "    lista = list(y_red[i])\n",
    "    y_pred.append(lista.index(max(y_red[i])))\n",
    "\n",
    "print(y_t[0:10])\n",
    "print(y_pred[0:10])\n",
    "    \n",
    "print(\"Matriz de Confusión:\")\n",
    "confusion_matrix(list(y_t), y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) SVM no lineal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se procede a entrenar una SVM no lineal, con la representación original ( pixeles sin pre-procesamiento ) y con la pre-procesada. Para modificar hiper-parámetros y tomar decisiones de diseño se utilizará el conjunto de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Miss Classification Loss sin procesamiento kernel rbf: 0.952103\n",
      "Miss Classification Loss sin procesamiento kernel poly: 0.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC as SVM\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# REPRESENTACIÓN ORIGINAL.\n",
    "\n",
    "model= SVM()\n",
    "model.set_params(C=0.01,kernel='rbf')\n",
    "model.fit(x_tr,y_tr)\n",
    "y_true = list(y_v)\n",
    "y_pred = list(model.predict(x_v))\n",
    "print(\"Miss Classification Loss sin procesamiento kernel rbf: %f\"%(1-accuracy_score(y_true, y_pred)))\n",
    "\n",
    "model= SVM()\n",
    "model.set_params(C=0.01,kernel='poly')\n",
    "model.fit(x_tr,y_tr)\n",
    "y_true = list(y_v)\n",
    "y_pred = list(model.predict(x_v))\n",
    "print(\"Miss Classification Loss sin procesamiento kernel poly: %f\"%(1-accuracy_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Miss Classification Loss con procesamiento kernel rbf: 0.645056\n",
      "Miss Classification Loss con procesamiento kernel poly: 0.750501\n"
     ]
    }
   ],
   "source": [
    "# REPRESENTACIÓN PRE-PROCESADA\n",
    "model= SVM()\n",
    "model.set_params(C=0.01,kernel='rbf')\n",
    "model.fit(px_tr,y_tr)\n",
    "y_true = list(y_v)\n",
    "y_pred = list(model.predict(px_v))\n",
    "print(\"Miss Classification Loss con procesamiento kernel rbf: %f\"%(1-accuracy_score(y_true, y_pred)))\n",
    "\n",
    "model= SVM()\n",
    "model.set_params(C=0.01,kernel='poly')\n",
    "model.fit(px_tr,y_tr)\n",
    "y_true = list(y_v)\n",
    "y_pred = list(model.predict(px_v))\n",
    "print(\"Miss Classification Loss con procesamiento kernel poly: %f\"%(1-accuracy_score(y_true, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f) Árbol de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuando con los método no-lineales, se procede a entrenar el árbol con la misma idea, con y sin pre-procesamiento, se ajustará la profundidad máxima en base al conjunto de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Miss Classification Loss Tree sin pre-procesamiento: 0.220543\n",
      "Miss Classification Loss Tree con pre-procesamiento: 0.218357\n"
     ]
    }
   ],
   "source": [
    "# Sin pre-procesamiento\n",
    "from sklearn.tree import DecisionTreeClassifier as Tree\n",
    "#Probarndo con n=10 , criterio entropy.\n",
    "\n",
    "model = Tree() #edit the train_model function\n",
    "model.set_params(max_depth=10,criterion='entropy',splitter='best')\n",
    "model.fit(x_tr,y_tr)\n",
    "y_true = list(y_v)\n",
    "y_pred = model.predict(x_v)\n",
    "print(\"Miss Classification Loss Tree sin pre-procesamiento: %f\"%(1-accuracy_score(y_true, y_pred)))\n",
    "\n",
    "# Pre procesado\n",
    "model = Tree() #edit the train_model function\n",
    "model.set_params(max_depth=10,criterion='entropy',splitter='best')\n",
    "model.fit(px_tr,y_tr)\n",
    "y_true = list(y_v)\n",
    "y_pred = model.predict(px_v)\n",
    "print(\"Miss Classification Loss Tree con pre-procesamiento: %f\"%(1-accuracy_score(y_true, y_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
